<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.92.2" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="" />
  <meta property="og:url" content="https://chamila.dev/blog/2019-11-25_elasticsearch-on-k8s-05-visualization-and-production-readying/" />
  <link rel="canonical" href="https://chamila.dev/blog/2019-11-25_elasticsearch-on-k8s-05-visualization-and-production-readying/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/chamila.dev\/"
      },
      "articleSection" : "blog",
      "name" : "ElasticSearch on K8s: 05 - Visualization and Production Readying",
      "headline" : "ElasticSearch on K8s: 05 - Visualization and Production Readying",
      "description" : "Visualization and Production Tweaks",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2019",
      "datePublished": "2019-11-25 22:29:57 \u002b1300 NZDT",
      "dateModified" : "2019-11-25 22:29:57 \u002b1300 NZDT",
      "url" : "https:\/\/chamila.dev\/blog\/2019-11-25_elasticsearch-on-k8s-05-visualization-and-production-readying\/",
      "keywords" : [ "K8s","Logging","ElasticSearch","Logstash","Filebeat","Log aggregation","ELKOnK8s", ]
  }
</script>
<title>ElasticSearch on K8s: 05 - Visualization and Production Readying - chamila.dev</title>
  <meta property="og:title" content="ElasticSearch on K8s: 05 - Visualization and Production Readying - chamila.dev" />
  <meta property="og:type" content="article" />
  <meta name="description" content="Visualization and Production Tweaks" />

  <link rel="stylesheet" href="https://unpkg.com/flexboxgrid@6.3.1/dist/flexboxgrid.min.css" />
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link rel="stylesheet" href="/css/light.css">
  <link rel="stylesheet" href="/css/dark.css">
  <link href="/blog/index.xml" rel="alternate" type="application/rss+xml" title="chamila.dev">

  <link href="/fa/css/all.css" rel="stylesheet">

  
  
  
  
  <script>
    

    (function (undefined) { }).call('object' === typeof window && window || 'object' === typeof self && self || 'object' === typeof global && global || {});
  </script>

  

</head>


<body class="theme-light">
  <article class="post " id="article">
    <div class="row">
      <div class="container col-xs-12 col-sm-10 col-md-8 col-sm-offset-1 col-md-offset-2 col-lg-6 col-lg-offset-3">
        <div id="floating-menu-wrapper">
  <div id="floating-menu">
    <button id="switch-to-dark">
      <i class="fas fa-moon"></i>
    </button>
    <button id="switch-to-light" class="current-theme">
      <i class="fas fa-sun"></i>
    </button>
  </div>
</div>

        <div class="site-header">
          
<header>
  <div class="signatures site-title">
    <span class="breadcrumbs">
      <a href="https://chamila.dev/">chamila.dev</a> >
      <a href="https://chamila.dev//blog"> journal </a> >
    </span>
    <div class="sm-icons">
  <a href="https://chamila.dev//blog/index.xml" target="_blank" title="rss">
    <i class="fas fa-rss sm-icon"></i>
  </a>
  <a href="https://github.com/chamilad" target="_blank" title="github">
    <i class="fab fa-github sm-icon"></i>
  </a>
  <a href="https://fosstodon.org/@chamilad" target="_blank" title="fosstodon" rel="me">
    <i class="fab fa-mastodon sm-icon"></i>
  </a>
</div>

  </div>
</header>
<div class="row end-xs">
   
</div>


        </div>
        <header class="post-header">
          <h1 class="post-title">ElasticSearch on K8s: 05 - Visualization and Production Readying</h1>
          
          <div class="row post-desc">
            <div class="pub-date col-xs-6">
              
              <time class="post-date" datetime=" 2019-11-25 22:29:57 NZDT">
                25 Nov 2019
              </time>
              
            </div>
            <div class="reading-time col-xs-6" title="approximate read time">
              ~12 minutes
            </div>
            
            
            
          </div>
          
          <div class="toc">
            
            <h4>Table of Contents:</h4>
            <nav id="TableOfContents">
  <ul>
    <li><a href="#01-buffering-at-logstash-layer">01: Buffering at Logstash Layer</a>
      <ul>
        <li><a href="#disk-based-buffering-for-incoming-events">Disk Based Buffering for Incoming Events</a></li>
        <li><a href="#throttling-events-at-logstash">Throttling Events at Logstash</a></li>
      </ul>
    </li>
    <li><a href="#02-keeping-elasticsearch-separate">02: Keeping ElasticSearch Separate</a></li>
    <li><a href="#03-managing-elasticsearch-jvm-parameters">03: Managing ElasticSearch JVM Parameters</a></li>
    <li><a href="#04-shard-count">04: Shard Count</a></li>
  </ul>
</nav>
            
          </div>
        </header>
        <div class="post-content markdown-body">
          <figure><img src="/blog/img/2019-11-25_elasticsearch-on-k8s-05_01.jpeg"/><figcaption>
            <h4>Woolshed Hut on Mt. Somers</h4>
        </figcaption>
</figure>

<blockquote>
<p>This is part of a series of short articles on setting up an ELK deployment on K8s.</p>
</blockquote>
<ol>
<li><a href="/elasticsearch-on-k8s-01-basic-design-ecfdaccbb63a">ElasticSearch on K8s: 01 — Basic Design</a></li>
<li><a href="/post/2019-09-21_elasticsearch-on-k8s-02log-collection-with-filebeat/">ElasticSearch on K8s: 02 — Log Collection with Filebeat</a></li>
<li><a href="/post/2019-11-22_elasticsearch-on-k8s-03-log-enrichment-with-logstash/">ElasticSearch on K8s: 03 - Log Enrichment with Logstash</a></li>
<li><a href="/post/2019-11-23_elasticsearch-on-k8s-04-log-storage-and-search-with-elasticsearch/">ElasticSearch on K8s: 04 - Log Storage and Search with ElasticSearch</a></li>
<li>ElasticSearch on K8s: 05 - Visualization and Production Readying</li>
<li><a href="/post/2019-11-26_elasticsearch-index-management/">ElasticSearch Index Management</a></li>
<li><a href="/post/2019-11-27_authentication-and-authorization-for-elasticsearch-01-a-blueprint-for-multi-tenant-sso/">Authentication and Authorization for ElasticSearch: 01 - A Blueprint for Multi-tenant SSO</a></li>
<li><a href="/post/2020-02-10_authentication-and-authorization-for-elasticsearch-02-basic-sso-with-role-assignment/">Authentication and Authorization for ElasticSearch: 02 - Basic SSO with Role Assignment</a></li>
<li><a href="/post/2020-02-12_authentication-and-authorization-for-elasticsearch-03-multi-tenancy-with-keycloak-and-kibana/">Authentication and Authorization for ElasticSearch: 03 - Multi-Tenancy with KeyCloak and Kibana</a></li>
</ol>
<h1 id="visualization">Visualization</h1>
<p>Kibana is a web application which can be used to query data from an ElasticSearch cluster. It does so through issuing queries to the ElasticSearch cluster through the REST API. Additionally it also acts as a widget and dashboard server that can be used to visualize specific queries, as well as a management frontend that can perform several actions like role and user management that are a lot cumbersome to do so through the ElasticSearch REST API. Kibana itself doesn’t have a requirement for persistence as it uses ElasticSearch itself as a persistence layer.</p>
<p>Being a web application, Kibana is the only tool out of this stack that has to be exposed to outside traffic in typical conditions (unless the ElasticSearch API is required to be exposed as well). Therefore, the attached <code>kibana</code> Service is linked to an Nginx Ingress which then can be exposed through a Cloud Service Provider load balancer.</p>
<blockquote>
<p>Exposing the stack to outside traffic inevitably means that some kind of application level authentication and authorization will have to be put in place. This will be discussed in a future article.</p>
</blockquote>
<p><img src="/blog/img/draft__clipboard_5.png#layoutTextWidth" alt=""></p>
<p>Since there are no special requirements for scheduling when it comes to Kibana, a typical K8s Deployment can be used to spawn Pods. As it turns out, this will also be the layer with the lowest user generated load (mostly because Kibana will be used during the time of an abnormal behavior in the system to debug and troubleshoot, situations which ideally should be rare. Having the bulk of the effective load for a request performed at the ElasticSearch layer also helps to reduce the load on the visualization tool).</p>
<p>Furthermore, being stateless on its own (since ElasticSearch is its persistence), Kibana Pods can be scaled up and down as desired without having to worry about persistence requirement.</p>
<p>When Kibana is first started up, it checks for an ElasticSearch index named <code>.kibana</code>, and creates one if none is found. This is the index that should be persisted between cluster upgrades. This index stores all the Kibana specific settings and artifacts (called <strong>Saved Objects</strong>).</p>
<blockquote>
<p>During high loads that may make ElasticSearch run out of resources, indices could get locked and marked as <strong>READ ONLY</strong> as a way to break input and stabilize the system. In this case the special system indices, including <code>.kibana</code> could also get locked. Unlocking these indices using the ElasticSearch API is an interesting exercise, highlighted by the fact that these lockouts occur during abnormal situations and you&rsquo;re on Kibana just to try to figure out what&rsquo;s going on in the first place.</p>
</blockquote>
<h1 id="gluing-the-stacktogether">Gluing the stack together</h1>
<p>Now that we have discussed each tool of the stack in detail, let’s take a closer look at how these components interact with each other.</p>
<p>Looking back at the first diagram <a href="/elasticsearch-on-k8s-01-basic-design-ecfdaccbb63a">on the first post of this series</a> which shows the complete logical deployment of the stack, the interactions between the components can be broken down to the following.</p>
<p><img src="/blog/img/2019-09-19_elasticsearch-on-k8s-01basic-design_1.png#layoutOutsetCenter" alt=""></p>
<ol>
<li>Filebeat talks to Logstash through the K8s Service <code>logstash</code> through port <code>5066</code> to push collected logs</li>
<li>Logstash talks to ElasticSearch through the K8s Service <code>elasticsearch</code> through the port <code>9200</code> to push enriched logs</li>
<li>ElasticSearch nodes in the cluster talk to each other using the K8s Headless Service <code>elasticsearch-headless</code> and the consistent Pod names through the port <code>9300</code> to form and maintain the cluster</li>
<li>Kibana, when the user initiates access, talks to ElasticSearch through the K8s Service <code>elasticsearch</code> through the HTTP port <code>9200</code> to read and write data</li>
</ol>
<p>The definitions of the Services mentioned above should reflect this by exposing the correct ports for communication. Furthermore, any <code>NetworkPolicy</code> definitions in place should also allow ingress and egress traffic from the parties defined in the above interactions.</p>
<p>For an example, when considering the interactions #2, #3, and #4, the <code>NetworkPolicy</code> attached to the ElasticSearch Pods should allow ingress traffic to port <code>9200</code> originating from labels attached to both Logstash and Kibana Pods. On the other hand, it should also restrict traffic to port <code>9300</code> so that only traffic originating from labels attached ElasticSearch.</p>
<p><code>kibana</code> Service should be exposed to outside traffic using an <a href="/load-balancing-and-reverse-proxying-for-kubernetes-services-f03dd0efe80">Nginx Ingress and attached Cloud Service Provider load balancers</a>. The port that the Service should expose is <code>5601</code> .</p>
<p>The above considerations should take care of most of the issues in connectivity between the Pods of the stack. However, this stack would not be ideal to be left out in the production environment. There will be layers that will easily buckle before sudden spikes or consistent high loads. They should be hardened in terms of fault tolerance.</p>
<h1 id="production-hardening">Production Hardening</h1>
<h2 id="01-buffering-at-logstash-layer">01: Buffering at Logstash Layer</h2>
<p>Filebeat and Logstash are essentially the content input and transformation layers of the stack. Therefore, naturally, the first line of defense against unwanted levels of input should be implemented at these layers. Implementing this at Filebeat could be simpler, by for example, excluding file patterns that may produce a large number of log lines per second. However that is not a proper adaptive defense against sources that unexpectedly send too large inputs, that otherwise may produce meaningful data. Therefore, it may be better to keep the complexity of buffering input away from Filebeat.</p>
<h3 id="disk-based-buffering-for-incoming-events">Disk Based Buffering for Incoming Events</h3>
<p>Logstash on the other hand could get affected first by a spike of input. “<a href="https://www.elastic.co/guide/en/logstash/current/persistent-queues.html#persistent-queues-architecture">Persistent Queues</a>” (PQs) is a built-in feature for Logstash to tackle this exact problem. In brief, Logstash instances with PQs enabled will keep a configurable file based buffer (instead of a fixed in-memory buffer) for incoming events to be processed. Events will clear this buffer only after being fully processed (i.e. going through <strong>filter</strong> and/or <strong>output</strong> stages, based on <strong>ACK</strong>nowledging the event to Filebeat as processed). If the buffer is filled at a certain points (ex: by a spike of inputs from one of the Filebeat instances) that Logstash instance will stop accepting events for processing, until the buffer is cleared to queue more input. This method of back-pressuring inputs lets Logstash gracefully fail during a spike, without dropping events.</p>
<p>PQs can be configured to have both a maximum event count and a maximum size on disk. Therefore, the buffer size can be experimented with to have the optimum size for managing back-pressure in a sensible manner.</p>
<p>For PQs, durability is important, and therefore, storing the PQs on a Pod storage, which does not survive Pod lifecycles, defeats the purpose. PQs should be complemented with Persistent Storage for Logstash. The easiest way to handle this is to convert the Logstash K8s Deployment to a StatefulSet with a <code>PersistentVolumeClaimTemplate</code> . This will make sure that PQs are stored in volumes that will not go down with the Pod or even the StatefulSet controller. This is the reason why Logstash component is portrayed as a StatefulSet managed set of Pods in the initial diagram.</p>
<p><img src="/blog/img/draft__clipboard_6.png#layoutTextWidth" alt=""></p>
<h3 id="throttling-events-at-logstash">Throttling Events at Logstash</h3>
<p>The above method of disk based buffering is only half of the work that can be done to throttle input at the Logstash layer. Provided that we have an understanding of the wanted and unwanted events, we can restrict the load on the stack furthermore. This is an active method of throttling as opposed to the passive buffering method described above. During buffering, the events that are not acked by Logstash will be tried again by Filebeat later. With throttling, the unwanted events are typically dropped and will not be evaluated again (unless otherwise fed again into the Logstash pipeline. This however will be rare since use of throttling will mostly happen for unwanted events).</p>
<p>How throttling is done at Logstash is using the <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-throttle.html">Throttle Filter</a>. It defines a window based on the number of events passing through the filter, so that events outside that window are throttled. This window is defined by the properties, <code>before_count</code> and <code>after_count</code>. This window is reset from time to time, which is also configurable using properties <code>period</code> and <code>max_age</code>. Furthermore, the throttle filter can be selectively applied for sources that are only likely to produce spikes of data. There could be multiple such window counters based on a <code>key</code> that is defined to separate events into diffeerent buckets.</p>
<p>For an example, to only allow a maximum of 1000 events from a single <code>webapp</code> Pod per minute, we can define a throttle filter like the following.</p>
<pre tabindex="0"><code>if [kubernetes][labels][app] in [&quot;webapp&quot;] {
  throttle {
    after_count =&gt; 1000
    key =&gt; &quot;%{kubernetes.pod.name}&quot;
    period =&gt; 60
    max_age =&gt; 120
    add_tag =&gt; &quot;throttled&quot;
  }
}
</code></pre><ol>
<li>The throttle filter is only engaged for events from the <code>webapp</code> labelled Pods</li>
<li>Throttling is engaged after a maximum of 1000 event counts</li>
<li>A window is defined per Pod name</li>
<li>The window is reset after 60 seconds</li>
<li>Each event that gets throttled is applied a tag named <code>throttled</code></li>
</ol>
<p>Based on this information, we can choose to drop any throttled event.</p>
<pre tabindex="0"><code>if &quot;throttled&quot; in [tags] { 
  drop {} 
}
</code></pre><blockquote>
<p>It&rsquo;s also possible to get the number of events throttled to get more visibility into the throttling functionality using a <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-metrics.html">Metrics Filter</a>.</p>
</blockquote>
<h2 id="02-keeping-elasticsearch-separate">02: Keeping ElasticSearch Separate</h2>
<p>As mentioned in another post in this series, ElasticSearch is the most resource intensive of the tools of this stack. Therefore, it is important to make sure that ElasticSearch Pods are getting adequate resources allocated to it and other non-related workloads are not affecting their performance drastically.</p>
<p>The ideal way to do this is to make sure that</p>
<ol>
<li>ElasticSearch Pods are scheduled into special types of Nodes with resources matching required performance</li>
<li>Other workloads are not getting scheduled in those special types of Nodes</li>
</ol>
<p>The K8s native way to implement this is through<a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/"> taints and tolerations</a>. The idea is to</p>
<ol>
<li>Spawn special VM Nodes to match the Pod replica count (ex: 2 in the sample case)</li>
<li>Mark the Nodes with a <strong>taint</strong> which is a key-value label</li>
<li>Mark the <code>elasticsearch</code> StatefulSet with <strong>tolerations</strong> that match the taint added to the Nodes</li>
</ol>
<p>This will make sure that only <code>elasticsearch</code> Pods that carry the matching tolerations, will get scheduled to the tainted Nodes.</p>
<p>For an example, the Nodes that are supposed to host the ElasticSearch Pods can be tainted with the following.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl taint nodes elasticsearch-node-01 role=elasticsearch:NoSchedule
kubectl taint nodes elasticsearch-node-02 role=elasticsearch:NoSchedule
</code></pre></div><p>The <code>elasticsearch</code> StatefulSet should get the following tolerations.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="font-weight:bold">tolerations</span>:
  - <span style="font-weight:bold">key</span>: <span style="color:#0ff;font-weight:bold">&#34;role&#34;</span>
    <span style="font-weight:bold">operator</span>: <span style="color:#0ff;font-weight:bold">&#34;Equal&#34;</span>
    <span style="font-weight:bold">value</span>: <span style="color:#0ff;font-weight:bold">&#34;elasticsearch&#34;</span>
    <span style="font-weight:bold">effect</span>: <span style="color:#0ff;font-weight:bold">&#34;NoSchedule&#34;</span>
</code></pre></div><p>After this change is done the Nodes will only carry the required <code>kube-system</code> Pods and <code>elasticsearch-*</code> Pods that match the tolerations. However, Filebeat Pods should also be scheduled in these Nodes for log collection to be completed. Therefore, the above tolerations that were added to ElasticSearch StatefulSet should also be added to the Filebeat DaemonSet.</p>
<figure><img src="/blog/img/draft__clipboard_7.png#layoutTextWidth"><figcaption>Physical distribution of Pods of the stack</figcaption></figure>
<blockquote>
<p>With this change, it’s important to make sure that PVs that are to be attached to the ElasticSearch Pods are spawned on the same regions as the targeted Nodes. This should be done by either matching the zones that the PVs will be spawned with the zones that ElasticSearch Nodes will be spawned in, or making sure that PVs are not spawned until the requests to spawn them are made. The first can be done by defining a <code>StorageClass</code> with the provisioner set to the required one (ex: <code>gp2</code> ) and using <code>allowedTopologies</code> to match the zones. How to achieve the latter is to define a custom <code>StorageClass</code> with the <code>volumeBindingMode</code> flag set to <code>WaitForFirstConsumer</code>. If such a step is not taken, there will be scenarios where <code>elasticsearch</code> Pods will be restricted to Nodes that do not have PVs spawned in that respective zone, failing the scheduling phase.</p>
</blockquote>
<h2 id="03-managing-elasticsearch-jvm-parameters">03: Managing ElasticSearch JVM Parameters</h2>
<p>ElasticSearch is a JVM. Being a memory intensive tool, it should be allocated enough memory so that</p>
<ol>
<li>the frequency of Garbage Collection is as less as possible</li>
<li>the Garbage Collection pause is as short as possible</li>
</ol>
<p>The JVM memory sizes (minimum and maximum) can be set through the use JVM configuration options provided by ElasticSearch. The exact values to set may vary on the type of the use. However as a best practice,</p>
<ol>
<li>match minimum and maximum heap sizes ( <code>Xms</code> and <code>Xmx</code> ) to reduce GC frequency early in the process uptime</li>
<li>match the Pod memory requests and limits to the heap size</li>
</ol>
<p>Since ElasticSearch Pods will be scheduled alone in a Node (because of the taint-toleration matching mentioned above) the JVM doesn’t have to share memory with other processes. Therefore, the heap size can comfortably be expanded to be close to the full amount of memory available to the Virtual Machine. The Pod memory requests and limits can also match these values since it’s guaranteed the Pods will be allowed to spawn in those specific Nodes.</p>
<h2 id="04-shard-count">04: Shard Count</h2>
<p>One key factor that may affect the actual amount of memory to be allocated to the ElasticSearch cluster will be the total number of Shards required to be available.</p>
<p>Each Index consists of at least 2 Shards if default settings are used. The total number of Shards in the cluster is restricted by the amount of memory available, along with the Shard specific factors such as Shard size. It’s a good idea to calculate the rough number of Shards required in the cluster, the general memory foot print of each Shard and then plan the amount of memory to be available to the cluster. A general rule of thumb would be to calculate around 30 Shards per 1GB of RAM, however this could easily change with.</p>
<ol>
<li>the size of the documents indexed</li>
<li>the frequency of document ingestion</li>
<li>the frequency of index rotation</li>
</ol>
<p>It should be noted that even with these considerations, the ElasticSearch cluster could still go OOM after a sudden, unexpected spike of input. This can be avoided by having necessary monitoring and alerting in place to know high memory usage in advance and scale the cluster as required.</p>
<blockquote>
<p>To calculate the number of Shards required to be running on a cluster, a data retention strategy should be defined. Let’s discuss how to do so in another article.</p>
</blockquote>

        </div>
        <div class="prev-next row">
	<div class="prev col-sm">
	
	<a href="https://chamila.dev/blog/2019-11-23_elasticsearch-on-k8s-04-log-storage-and-search-with-elasticsearch/">&lt; ElasticSearch on K8s: 04 - Log Storage and Search with ElasticSearch</a>
		
	</div>        
	<div class="next col-sm">
	
	<a href="https://chamila.dev/blog/2019-11-26_elasticsearch-index-management/">ElasticSearch Index Management &gt;</a>
		
	</div>
</div>


        

        

<div class="releated-content">
  <h3>Related Posts</h3>
  <ul>
    
    <li><a href="/blog/2019-11-23_elasticsearch-on-k8s-04-log-storage-and-search-with-elasticsearch/">ElasticSearch on K8s: 04 - Log Storage and Search with ElasticSearch</a></li>
    
    <li><a href="/blog/2019-11-22_elasticsearch-on-k8s-03-log-enrichment-with-logstash/">ElasticSearch on K8s: 03 - Log Enrichment with Logstash</a></li>
    
    <li><a href="/blog/2019-09-21_elasticsearch-on-k8s-02log-collection-with-filebeat/">ElasticSearch on K8s: 02 — Log Collection with Filebeat</a></li>
    
  </ul>
</div>


        
        
        
        
        

        <div class="site-footer">
  <div class="sm-icons">
  <a href="https://chamila.dev//blog/index.xml" target="_blank" title="rss">
    <i class="fas fa-rss sm-icon"></i>
  </a>
  <a href="https://github.com/chamilad" target="_blank" title="github">
    <i class="fab fa-github sm-icon"></i>
  </a>
  <a href="https://fosstodon.org/@chamilad" target="_blank" title="fosstodon" rel="me">
    <i class="fab fa-mastodon sm-icon"></i>
  </a>
</div>

  <span> All photos published on this site are copyrighted. </span>
  <div class="site-footer-item">
    Modified
    <a href="https://github.com/joway/hugo-theme-yinyang" target="_blank">YinYang</a>
    theme
  </div>
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>
<script src="/js/theme.js"></script>


<script>
  hljs.initHighlightingOnLoad();

  var posts = document.getElementById('posts-list');
  posts && quicklink({
    el: posts,
    priority: true,
  });
</script>

  

</body>

</html>
